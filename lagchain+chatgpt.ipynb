{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "E9ng5uAAUlFr"
      },
      "outputs": [],
      "source": [
        "!pip install langchain\n",
        "!pip install python-magic-bin==0.4.14\n",
        "!pip install nltk\n",
        "!pip install unstructured\n",
        "!pip install openai\n",
        "!pip install chromadb\n",
        "!pip install tiktoken\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain import OpenAI, VectorDBQA\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "\n",
        "import magic\n",
        "import os\n",
        "import nltk\n",
        "\n",
        "\n",
        "%env OPENAI_API_KEY=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\n",
        "%env SERPAPI_API_KEY=cf0cf710adb0302c2344075c5c9b92f9cb08613154686192011da1d5f562f1f6"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kc682LLAVYoi",
        "outputId": "3bd96105-68ed-4c5c-a6ea-0f75da388d0a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: OPENAI_API_KEY=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\n",
            "env: SERPAPI_API_KEY=cf0cf710adb0302c2344075c5c9b92f9cb08613154686192011da1d5f562f1f6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "openai.api_key = os.environ['OPENAI_API_KEY']\n",
        "model_meta = openai.Model.list()"
      ],
      "metadata": {
        "id": "CSJ5EKUd_moJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [x.id for x in model_meta['data']]"
      ],
      "metadata": {
        "id": "4-6dENgB_mlT"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = DirectoryLoader('data/', glob='**/*.txt')\n",
        "documents = loader.load()"
      ],
      "metadata": {
        "id": "od5ikqhVVb6K"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "texts = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "ReqauYIDVfmD"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = OpenAIEmbeddings(openai_api_key=os.environ['OPENAI_API_KEY'])\n",
        "docsearch = Chroma.from_documents(texts, embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57Kg2PnxVh9V",
        "outputId": "fa2a4b33-7f95-4681-b645-4c8d8d371217"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:chromadb:Using embedded DuckDB without persistence: data will be transient\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_model = OpenAI(model_name=\"text-davinci-003\", openai_api_key=os.environ['OPENAI_API_KEY'])"
      ],
      "metadata": {
        "id": "3eUBxc99RAax"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa = VectorDBQA.from_chain_type(llm=llm_model, chain_type=\"stuff\", vectorstore=docsearch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TvkLXhBkpaL",
        "outputId": "d4dc5b7a-7114-4457-995e-5d19ebe24ad4"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/langchain/chains/retrieval_qa/base.py:185: UserWarning: `VectorDBQA` is deprecated - please use `from langchain.chains import RetrievalQA`\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# query = \"What did McCarthy discover?\"\n",
        "# qa.run(query)"
      ],
      "metadata": {
        "id": "F7V9cfw0kpQL"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"list down the Principles of Insurance?\"\n",
        "print(qa.run(query))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJPbIQqxkpKd",
        "outputId": "e07366ed-8254-400b-a6cc-ad4a2ea5e600"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " \n",
            "1. Principle of Utmost good faith \n",
            "2. Principle of Insurable interest \n",
            "3. Principle of Indemnity \n",
            "4. Principle of Subrogation \n",
            "5. Principle of Contribution \n",
            "6. Principle of Proximate cause \n",
            "7. Principle of Loss of Minimization\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BmKVkgoDkpCM"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import openai\n",
        "# openai.api_key = os.environ['OPENAI_API_KEY']\n",
        "# messages = [ {\"role\": \"system\", \"content\": \n",
        "#               \"You are a intelligent assistant.\"} ]\n",
        "# while True:\n",
        "#     message = input(\"User : \")\n",
        "#     if message:\n",
        "#         messages.append(\n",
        "#             {\"role\": \"user\", \"content\": message},\n",
        "#         )\n",
        "#         chat = openai.ChatCompletion.create(\n",
        "#             model=\"gpt-3.5-turbo\", messages=messages\n",
        "#         )\n",
        "#     reply = chat.choices[0].message.content\n",
        "#     print(f\"ChatGPT: {reply}\")\n",
        "#     messages.append({\"role\": \"assistant\", \"content\": reply})"
      ],
      "metadata": {
        "id": "iJYq5wPMVlVd"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prompt Templates: Manage prompts for LLMs**"
      ],
      "metadata": {
        "id": "cmqdu5yBAxv0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "\n",
        "llm = OpenAI(temperature=0.9)\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"food\"],\n",
        "    template=\"What are 5 vacation destinations for someone who likes to eat {food}?\",\n",
        ")\n",
        "print(prompt.format(food=\"dessert\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZlDnAO2oVpxx",
        "outputId": "e68ce52b-6fec-42cf-c1a5-cb0d6cb65f48"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What are 5 vacation destinations for someone who likes to eat dessert?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(llm(prompt.format(food=\"dessert\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTc6CWe42eIa",
        "outputId": "d6c7552b-5fb4-4d70-ca54-07d16f8dfc83"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "1. Macau, China: for its famous egg tarts.\n",
            "2. Amsterdam, The Netherlands: for the pancakes and stroopwafels.\n",
            "3. Vienna, Austria: for its famous apple strudel and other Viennese pastries.\n",
            "4. Paris, France: for its Ã©clairs, macarons, and other delicious desserts.\n",
            "5. Tokyo, Japan: for its mochi and other traditional Japanese sweets.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CICWZSkQHaEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain import PromptTemplate\n",
        "\n",
        "llm = OpenAI(model_name=\"text-davinci-003\", openai_api_key=os.environ['OPENAI_API_KEY'])\n",
        "\n",
        "# Notice \"location\" below, that is a placeholder for another value later\n",
        "template = \"\"\"\n",
        "I really want to travel to {location}. What should I do there?\n",
        "\n",
        "Respond in one short sentence\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"location\"],\n",
        "    template=template,\n",
        ")\n",
        "\n",
        "final_prompt = prompt.format(location='Rome')\n",
        "\n",
        "print (f\"Final Prompt: {final_prompt}\")\n",
        "print (\"-----------\")\n",
        "print (f\"LLM Output: {llm(final_prompt)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VztZLaDLHaAQ",
        "outputId": "90d1867a-721e-411c-84a7-cdcacf663c47"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Prompt: \n",
            "I really want to travel to Rome. What should I do there?\n",
            "\n",
            "Respond in one short sentence\n",
            "\n",
            "-----------\n",
            "LLM Output: Visit the Colosseum, the Pantheon, and the Trevi Fountain.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agents: Dynamically call chains based on user input"
      ],
      "metadata": {
        "id": "qpeCk3wuBIlJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install google-search-results"
      ],
      "metadata": {
        "id": "fEkwHmF1AsPj"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import load_tools\n",
        "from langchain.agents import initialize_agent\n",
        "from langchain.llms import OpenAI\n",
        "# Load the model\n",
        "llm = OpenAI(temperature=0)"
      ],
      "metadata": {
        "id": "R3L6sA39BOWd"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)"
      ],
      "metadata": {
        "id": "dhkPb7nlBR7B"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finally, let's initialize an agent with:\n",
        "# 1. The tools\n",
        "# 2. The language model\n",
        "# 3. The type of agent we want to use.\n",
        "\n",
        "\n",
        "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)"
      ],
      "metadata": {
        "id": "zHK4wSjGBUWf"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "See list of agents types [here](https://langchain.readthedocs.io/en/latest/modules/agents/agents.html?highlight=zero-shot-react-description)"
      ],
      "metadata": {
        "id": "0s-0NLBcDAex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now let's test it out!\n",
        "agent.run(\"Who is the current leader of Japan? What is the largest prime number that is smaller than their age?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "FFK7qlsrC8tp",
        "outputId": "5b965ed1-6f34-4f8a-861e-b39ddae79aa8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I need to find out who the leader of Japan is and then calculate the largest prime number that is smaller than their age.\n",
            "Action: Search\n",
            "Action Input: \"current leader of Japan\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mFumio Kishida\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I need to find out the age of the leader of Japan\n",
            "Action: Search\n",
            "Action Input: \"age of Fumio Kishida\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m65 years\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I need to calculate the largest prime number that is smaller than 65\n",
            "Action: Calculator\n",
            "Action Input: 65\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3mAnswer: 65\n",
            "\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
            "Final Answer: The current leader of Japan is Fumio Kishida and the largest prime number that is smaller than their age is 61.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The current leader of Japan is Fumio Kishida and the largest prime number that is smaller than their age is 61.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YomcF2cxDODG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Simple Sequential Chains\n",
        "Easy chains where you can use the output of an LMM as an input into another. Good for breaking up tasks (and keeping your LLM focused)"
      ],
      "metadata": {
        "id": "bhSBRLapOInq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import SimpleSequentialChain\n",
        "\n",
        "llm = OpenAI(temperature=1, openai_api_key=os.environ['OPENAI_API_KEY'])\n",
        "template = \"\"\"Your job is to come up with a classic dish from the area that the users suggests.\n",
        "% USER LOCATION\n",
        "{user_location}\n",
        "\n",
        "YOUR RESPONSE:\n",
        "\"\"\"\n",
        "prompt_template = PromptTemplate(input_variables=[\"user_location\"], template=template)\n",
        "\n",
        "# Holds my 'location' chain\n",
        "location_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
        "template = \"\"\"Given a meal, give a short and simple recipe on how to make that dish at home.\n",
        "% MEAL\n",
        "{user_meal}\n",
        "\n",
        "YOUR RESPONSE:\n",
        "\"\"\"\n",
        "prompt_template = PromptTemplate(input_variables=[\"user_meal\"], template=template)\n",
        "\n",
        "# Holds my 'meal' chain\n",
        "meal_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
        "overall_chain = SimpleSequentialChain(chains=[location_chain, meal_chain], verbose=True)\n",
        "review = overall_chain.run(\"Rome\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fe9BXF8_Ly0r",
        "outputId": "397f6be0-3775-4ed6-de1c-a2678fcbd678"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
            "\u001b[36;1m\u001b[1;3mPasta alla Carbonara.\u001b[0m\n",
            "\u001b[33;1m\u001b[1;3mIngredients:\n",
            "- 6 slices of bacon\n",
            "- 2 cloves of garlic\n",
            "- 2 tablespoons of olive oil\n",
            "- 2 cups of spaghetti\n",
            "- 4 eggs\n",
            "- Â½ cup of Parmesan cheese\n",
            "- Salt and pepper to taste\n",
            "\n",
            "Directions:\n",
            "1. Cook the bacon in a pan over medium-high heat for about 10 minutes or until itâs crisp. Set aside on a paper towel to drain.\n",
            "2. Mince the garlic and add it to the same pan with the olive oil. Cook for about 1 minute.\n",
            "3. Cook the spaghetti per package directions.\n",
            "4. In a bowl, beat the eggs and mix it with the Parmesan cheese, salt and pepper.\n",
            "5. Once the spaghetti is cooked, drain it and add it to the pan with the garlic and olive oil. Cook for 1 minute.\n",
            "6. Add the bacon to the pan and stir until itâs combined.\n",
            "7. Add the egg and cheese mixture, stirring quickly and constantly until itâs all melted together.\n",
            "8. Enjoy!\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XuIY1p4BIh_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Example Selectors**\n",
        "An easy way to select from a series of examples that allow you to dynamic place in-context information into your prompt. Often used when your task is nuanced or you have a large list of examples.\n",
        "\n",
        "Check out different types of example selectors [here](https://python.langchain.com/en/latest/modules/prompts/example_selectors.html)"
      ],
      "metadata": {
        "id": "ywDaiaCWKEKa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install faiss-cpu"
      ],
      "metadata": {
        "id": "Xf5Z7HLcKFXG"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm = OpenAI(model_name=\"text-davinci-003\", openai_api_key=os.environ['OPENAI_API_KEY'])\n",
        "\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"input\", \"output\"],\n",
        "    template=\"Example Input: {input}\\nExample Output: {output}\",\n",
        ")\n",
        "\n",
        "# Examples of locations that nouns are found\n",
        "examples = [\n",
        "    {\"input\": \"pirate\", \"output\": \"ship\"},\n",
        "    {\"input\": \"pilot\", \"output\": \"plane\"},\n",
        "    {\"input\": \"driver\", \"output\": \"car\"},\n",
        "    {\"input\": \"tree\", \"output\": \"ground\"},\n",
        "    {\"input\": \"bird\", \"output\": \"nest\"},\n",
        "]\n",
        "\n",
        "\n",
        "# SemanticSimilarityExampleSelector will select examples that are similar to your input by semantic meaning\n",
        "\n",
        "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
        "    # This is the list of examples available to select from.\n",
        "    examples, \n",
        "    \n",
        "    # This is the embedding class used to produce embeddings which are used to measure semantic similarity.\n",
        "    OpenAIEmbeddings(openai_api_key=os.environ['OPENAI_API_KEY']), \n",
        "    \n",
        "    # This is the VectorStore class that is used to store the embeddings and do a similarity search over.\n",
        "    FAISS, \n",
        "    \n",
        "    # This is the number of examples to produce.\n",
        "    k=2\n",
        ")\n",
        "\n",
        "similar_prompt = FewShotPromptTemplate(\n",
        "    # The object that will help select examples\n",
        "    example_selector=example_selector,\n",
        "    \n",
        "    # Your prompt\n",
        "    example_prompt=example_prompt,\n",
        "    \n",
        "    # Customizations that will be added to the top and bottom of your prompt\n",
        "    prefix=\"Give the location an item is usually found in\",\n",
        "    suffix=\"Input: {noun}\\nOutput:\",\n",
        "    \n",
        "    # What inputs your prompt will receive\n",
        "    input_variables=[\"noun\"],\n",
        ")\n",
        "\n",
        "\n",
        "# Select a noun!\n",
        "my_noun = \"hen\"\n",
        "\n",
        "print(similar_prompt.format(noun=my_noun))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AztdlZFNIh0p",
        "outputId": "0a5bbaf7-123d-41f5-a9bb-53d44c6bb1e8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Give the location an item is usually found in\n",
            "\n",
            "Example Input: bird\n",
            "Example Output: nest\n",
            "\n",
            "Example Input: driver\n",
            "Example Output: car\n",
            "\n",
            "Input: hen\n",
            "Output:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm(similar_prompt.format(noun=my_noun))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "zOxLhqTuImlj",
        "outputId": "0769c7d7-f113-49c8-a4cb-40d7e3b819f9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' coop'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "8qEB569DI2zD",
        "outputId": "82f9c5f7-1369-470e-da06-feb8143f0d78"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ekVuIFbaLywt"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NTl3fJR4L5O3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}